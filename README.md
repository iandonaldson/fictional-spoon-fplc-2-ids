# fictional-spoon-fplc-2-ids

## Overview

This repository is a proof-of-concept and initial exploratory investigation of FPLC (Fast Protein Liquid Chromatography) files made by the AKTA Pure system running UNICORN software.  The primary goal was to develop parsers that convert various FPLC formats to an Intermediary Data Schema (IDS) written in json using a "DOE" development scheme (directive-orchestration-execution) that used Github Codespaces, Co-pilot and Claude Sonnet 4.5 (see .github/copilot-instructions).  Initial problem definition and search for appropriate starting data and solutions was carried out using ChatGPT 5.2 (Deep Research).  THIS IS NOT A PRODUCTION CODEBASE, but represents a useful starting point to collect initial data sets, observations of data structures and potential solutions in one place.  There are two tagged versions of this repository.  

**Session_1**: 7 hours development time including initial background research.  
State of code after the first pass solution.  See notes/ongoing_developer_notes.md for an abbreviated recounting.  This pass likely produced a successful capture of trace data according to excution/test_extraction_coverage.py. and work in Session_2 but this needs to be manually confirmed.  
  
**Session_2**: 4 hours development time.  
Whilst extraction of trace data looks reasonable on initial inspection, review of Session_1 output focused on binary metadata and found that this is in compressed archives of XML data with .NET headers.  THESE METADATA ARE NOT PROPERLY EXTRACTED BY THE CURRENT CODEBASE.  A solution going forward is proposed.  Findings and suggestions for next steps are covered in notes/Session2 - review seesion 1 output and in notes/ongoing_developer_notes.md.  These notes are, in themselves, a useful description of the initial data structure which I was not able to find elsewhere.

**Conclusions**:  
Use of Github Co-pilot and Claude Sonnet 4.5 and the DOE method (see .github/copilot-instructions.md) was used by a developer with no prior experience in this domain (FPLC/AKTA/UNICORN), to develop a working software repository that processed FPLC AKTA data generated by the UNICORN software.  This is a non-trivial parsing problem based on a proprietary and legacy standard.  The repo contains initial working code, tests of output and documentation that was generated in less than eight hours.  Additional sessions will be required to validate the code; however, the results of the initial inspection are promising and these exploratory sessions have identified the crux of the problem going forward and represent a level of developer augmentation that would not have been possible mere months ago.      

IMD 2026-01-18 

## Acknowledgements  
This code repository made use of two prior code repositories that are no longer under active development at the time of writing.  
https://github.com/pyahmed/PyCORN    
https://github.com/ronald-jaepel/PyCORN  
  
## Quick Start 

### Prerequisites 

- Python 3.x
- PyCORN library (installed automatically by pipeline)

### Running the Complete Pipeline

```bash
# Clean build with all validation (recommended for first run)
python orchestrate.py --clean

# Process specific files only
python orchestrate.py --process-files "sample.zip,file2.zip"

# Skip specific validation steps
python orchestrate.py --no-check-end2end

# Custom data directory
python orchestrate.py --data-dir path/to/data
```

## Pipeline Architecture

The pipeline follows a 6-step process:

1. **Extract** - Extract data from AKTA .zip archives using PyCORN
2. **Test Extraction** - Verify all source files were extracted successfully
3. **Convert** - Transform extracted data to IDS JSON format
4. **Validate** - Verify IDS conversions preserve all data
5. **End-to-End Test** - Complete pipeline coverage validation (optional)
6. **CSV Export** - Generate CSV files from IDS data for analysis

### Pipeline Orchestrator

The `orchestrate.py` script coordinates the entire pipeline with configurable options:

```bash
python orchestrate.py [options]

Options:
  --data-dir PATH           Data directory with AKTA .zip files (default: data/akta)
  --process-files FILES     Files to process: 'all', 'none', or comma-separated list
  --log-dir PATH            Directory for timestamped logs (default: output/logs)
  --clean                   Clean all output directories before starting
  --csv / --no-csv          Create CSV exports (default: yes)
  --check-extraction        Verify extraction coverage (default: yes)
  --check-conversion        Validate IDS conversions (default: yes)
  --check-end2end           Run end-to-end pipeline test (default: yes)
```

### Directory Structure

```
fictional-spoon-fplc-2-ids/
├── orchestrate.py              # Main pipeline coordinator
├── data/
│   └── akta/                   # Source AKTA .zip files
├── execution/                  # Individual processing scripts
│   ├── extract_akta.py         # AKTA data extraction
│   ├── akta_to_ids.py          # IDS conversion + CSV export
│   ├── test_extraction_coverage.py
│   ├── validate_ids_conversion.py
│   └── test_complete_pipeline.py
├── directives/                 # Process documentation
│   ├── a2i.md                  # AKTA to IDS converter directive
│   ├── ids_schema_v1.json      # IDS JSON schema definition
│   ├── IDS_DOCUMENTATION.md    # Schema documentation
│   └── PyCORN_usage.md         # PyCORN API reference
├── .tmp/
│   └── akta_extracted/         # Temporary extraction files
│       └── {sample}/
│           ├── raw_files/      # Original extracted files from .zip
│           ├── {sample}_extracted.json
│           └── {sample}_summary.json
└── output/
    ├── logs/                   # Timestamped execution logs
    └── {sample}/               # Final outputs per sample
        ├── json/               # IDS JSON files
        │   └── {sample}.ids.json
        └── csv/                # CSV exports
            └── {sample}.ids.csv
```

## Manual Execution

Individual pipeline steps can be run manually:

### 1. Extract AKTA Data

```bash
python execution/extract_akta.py --all .tmp/akta_extracted
```

### 2. Test Extraction Coverage

```bash
python execution/test_extraction_coverage.py
```

### 3. Convert to IDS Format

```bash
python execution/akta_to_ids.py path/to/extracted.json
# Or convert all files in a directory
python execution/akta_to_ids.py --all .tmp/akta_extracted
```

### 4. Validate IDS Conversion

```bash
python execution/validate_ids_conversion.py
```

### 5. Generate CSV Export

```bash
python execution/akta_to_ids.py --csv path/to/file.ids.json
```

### 6. Run Complete Pipeline Test

```bash
python execution/test_complete_pipeline.py
```

## IDS Format

The Intermediary Data Schema (IDS) is a standardized JSON format for chromatography data:

- **Schema Version**: 1.0.0
- **Multi-platform support**: AKTA, BioRad, and other FPLC systems
- **Flexible sensor data**: Variable sampling rates per sensor
- **Complete metadata**: Provenance, instrument config, run parameters
- **Event tracking**: Injections, fractions, alarms, user marks
- **Optional peaks**: Integrated peak data when available

See [directives/IDS_DOCUMENTATION.md](directives/IDS_DOCUMENTATION.md) for complete specification.

## Logging

All pipeline runs generate timestamped logs in `output/logs/`:

- `orchestrate_YYYYMMDD_HHMMSS.log` - Main orchestration log
- `step{N}_{operation}_YYYYMMDD_HHMMSS.log` - Individual step logs
- `results_YYYYMMDD_HHMMSS.json` - Machine-readable results summary

## Testing

The pipeline includes comprehensive testing at multiple levels:

- **Unit tests**: Individual file extraction and conversion
- **Integration tests**: Cross-step data validation
- **End-to-end tests**: Complete pipeline coverage

Run all tests with:
```bash
python orchestrate.py --clean
```

## Known Issues

1. See **Overview**

## Documentation

- [directives/a2i.md](directives/a2i.md) - Complete AKTA to IDS process documentation
- [directives/PyCORN_usage.md](directives/PyCORN_usage.md) - PyCORN API reference
- [directives/IDS_DOCUMENTATION.md](directives/IDS_DOCUMENTATION.md) - IDS schema specification
- [.github/copilot-instructions.md](.github/copilot-instructions.md) - AI agent instructions

## License

See [LICENSE](LICENSE) for details.
